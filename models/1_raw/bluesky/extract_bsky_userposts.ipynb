{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atproto import Client\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# write a chunk of post-data to CSV\n",
    "def write_chunk(df: str, target_user: str) -> None:\n",
    "    # filename format is <username>_<earliest_contained_post_date>_<latest_contained_post_date>\n",
    "    start = df['post_created_at'].min().to_pydatetime().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    end = df['post_created_at'].max().to_pydatetime().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    filename = f\"usr_{target_user}/{target_user}.bsky.social_from_{start}_to_{end}.csv\"\n",
    "    df.to_csv(filename)\n",
    "\n",
    "# ingest all posts for a specific BlueSky user as a collection of CSVs\n",
    "def ingest_posts_by_user(target_user: str, output_filename: str) -> None:\n",
    "    # Retrieve login info from local env\n",
    "    USR = os.getenv('BSY_USR').lower()\n",
    "    KEY = os.getenv('BSY_KEY')\n",
    "    # Instantiate a BlueSky session\n",
    "    cli = Client()\n",
    "    cli.login(USR, KEY)\n",
    "\n",
    "    schema = {'content_id':                []\n",
    "             ,'post_uri':                  []\n",
    "             ,'like_count':                []\n",
    "             ,'quote_count':               []\n",
    "             ,'reply_count':               []\n",
    "             ,'repost_count':              []\n",
    "             ,'post_created_at':           []\n",
    "             ,'text':                      []\n",
    "             ,'tags':                      []\n",
    "             ,'embedded_link_title':       []\n",
    "             ,'embedded_link_description': []\n",
    "             ,'embedded_link_uri':         []\n",
    "             ,'author_username':           []\n",
    "             ,'author_displayname':        []\n",
    "             ,'author_account_created_at': []\n",
    "             }\n",
    "    data = schema \n",
    "    \n",
    "    # Set some control vars for ingestion\n",
    "    pages_remain = True\n",
    "    page_num = 0\n",
    "    csr = None\n",
    "    filenum = 0\n",
    "    \n",
    "    # Iterate through every post in their account's post history\n",
    "    while pages_remain:\n",
    "        \n",
    "        # check if the current file is already \"full\" (larger than 100 MB)\n",
    "        # if it is, stash the current data object as CSV and reset a new empty one\n",
    "        df = pd.DataFrame(data)\n",
    "        if df.memory_usage(deep=True).sum() / (1024*1024) >= 100:\n",
    "            filenum+=1\n",
    "            write_chunk(df, target_user)\n",
    "            data = schema\n",
    "            del df # No need to lock up memory while the next instance of `data` is filling up...\n",
    "        \n",
    "        # Retrieve a paginated post-feed for a specific bluesky user\n",
    "        page_num +=1  \n",
    "        resp = cli.get_author_feed(target_user, cursor=csr)\n",
    "        feed = resp.feed\n",
    "        print(f\"Retrieving post data from page {page_num} for user @{target_user}.bsky.social...\", end='\\r')\n",
    "        for item in feed:\n",
    "            # i drink your data! i DRINK IT UP ლಠ益ಠ)ლ\n",
    "            data['content_id'].append(item.post.cid)\n",
    "            data['post_uri'].append(item.post.uri)\n",
    "            data['like_count'].append(item.post.like_count)\n",
    "            data['quote_count'].append(item.post.quote_count)\n",
    "            data['reply_count'].append(item.post.reply_count)\n",
    "            data['repost_count'].append(item.post.repost_count)\n",
    "            data['post_created_at'].append(item.post.record.created_at)\n",
    "            data['text'].append(item.post.record.text)\n",
    "            data['tags'].append(item.post.record.tags)\n",
    "            # post may or may not have external links\n",
    "            try:\n",
    "                data['embedded_link_title'].append(item.post.record.embed.external.title)\n",
    "            except AttributeError:\n",
    "                data['embedded_link_title'].append('null')\n",
    "            try:\n",
    "                data['embedded_link_description'].append(item.post.record.embed.external.description)\n",
    "            except AttributeError:\n",
    "                data['embedded_link_description'].append('null')\n",
    "            try:\n",
    "                data['embedded_link_uri'].append(item.post.record.embed.external.uri)\n",
    "            except AttributeError:\n",
    "                data['embedded_link_uri'].append('null')\n",
    "            data['author_username'].append(item.post.author.handle)\n",
    "            data['author_displayname'].append(item.post.author.display_name)\n",
    "            data['author_account_created_at'].append(item.post.author.created_at)\n",
    "        \n",
    "        if not resp.cursor:\n",
    "            pages_remain = False # flag to stop ingestion when the final page of posts is reached \n",
    "        csr = resp.cursor        # reset cursor when another page of posts is available\n",
    "        time.sleep(2)            # limit query rate to avoid throttling, etc\n",
    "    write_chunk(df, target_user)\n",
    "    print(f\"Post Ingestion for user @{target_user}.bsky.social Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Retrieving post data from page 6 for user @politico.com.bsky.social...\n",
      ">Writing to disk as feed_output.csv...\n",
      ">Post Ingestion for user @politico.com.bsky.social Complete!\n"
     ]
    }
   ],
   "source": [
    "ingest_posts_by_user('politico.com', 'feed_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-12 12:30:30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "data = {'datecol': [datetime(2025, 3, 10, 12, 30, 30), datetime(2025, 3, 11, 12, 30, 30), datetime(2025, 3, 12, 12, 30, 30), datetime(2025, 3, 10, 13, 30, 30)]}\n",
    "df = pd.DataFrame(data)\n",
    "df['datecol'].max().to_pydatetime().strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ''\n",
    "len(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
